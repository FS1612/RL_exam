{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10272857,"sourceType":"datasetVersion","datasetId":6356252},{"sourceId":10277152,"sourceType":"datasetVersion","datasetId":6359174}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport matplotlib.pyplot as plt\nimport os\n\n# Define Maze Environment with Curriculum Learning\nclass CustomMazeEnv:\n    def __init__(self, size=15, initial_obstacles=5, max_obstacles=50):\n        \"\"\"\n        Initialize the maze environment.\n\n        Args:\n            size (int): Size of the maze (NxN grid).\n            initial_obstacles (int): Initial number of obstacles.\n            max_obstacles (int): Maximum number of obstacles.\n        \"\"\"\n        self.size = size\n        self.state_dim = (3, size, size)  # Channels for agent, goal, obstacles\n        self.action_dim = 4  # Actions: 0 = up, 1 = down, 2 = left, 3 = right\n        self.initial_obstacles = initial_obstacles\n        self.max_obstacles = max_obstacles\n        self.current_obstacles = initial_obstacles\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Reset the maze environment by repositioning the agent and goal\n        and regenerating obstacles.\n\n        Returns:\n            state (array): Initial state of the environment.\n        \"\"\"\n        while True:\n            self.agent_pos = np.random.randint(0, self.size, size=2)\n            self.goal_pos = np.random.randint(0, self.size, size=2)\n            if not np.array_equal(self.agent_pos, self.goal_pos):\n                self._generate_maze()\n                if self._is_path_possible():\n                    break\n\n        self.steps = 0\n        self.prev_agent_pos = self.agent_pos.copy()\n        return self._get_state()\n\n    def increase_difficulty(self):\n        \"\"\"\n        Increase the number of obstacles to make the maze more difficult.\n        \"\"\"\n        self.current_obstacles = min(self.current_obstacles + 1, self.max_obstacles)\n\n    def _generate_maze(self):\n        \"\"\"\n        Generate a random maze with obstacles.\n        \"\"\"\n        while True:\n            self.obstacles = []\n            maze = np.zeros((self.size, self.size), dtype=int)\n            maze[self.agent_pos[0], self.agent_pos[1]] = 1\n            maze[self.goal_pos[0], self.goal_pos[1]] = 1\n\n            for i in range(self.size):\n                for j in range(self.size):\n                    if maze[i, j] == 0 and np.random.rand() < self.current_obstacles / (self.size ** 2):\n                        self.obstacles.append((i, j))\n\n            if self._is_path_possible():\n                break\n\n    def _is_path_possible(self):\n        \"\"\"\n        Check if there is a valid path from the agent's position to the goal.\n        Uses depth-first search (DFS).\n\n        Returns:\n            bool: True if a path exists, False otherwise.\n        \"\"\"\n        def dfs(x, y, visited):\n            if (x, y) == tuple(self.goal_pos):\n                return True\n            visited.add((x, y))\n            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < self.size and 0 <= ny < self.size and (nx, ny) not in visited and (nx, ny) not in self.obstacles:\n                    if dfs(nx, ny, visited):\n                        return True\n            return False\n\n        return dfs(self.agent_pos[0], self.agent_pos[1], set())\n\n    def _get_state(self):\n        \"\"\"\n        Return the current state of the environment as a 3D array.\n        - Channel 0: Agent position.\n        - Channel 1: Goal position.\n        - Channel 2: Obstacles.\n\n        Returns:\n            state (array): 3D array representation of the environment.\n        \"\"\"\n        state = np.zeros(self.state_dim, dtype=np.float32)\n        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n        state[1, self.goal_pos[0], self.goal_pos[1]] = 1.0\n        for obs in self.obstacles:\n            state[2, obs[0], obs[1]] = 1.0\n        return state\n\n    def step(self, action):\n        \"\"\"\n        Execute an action in the maze.\n\n        Args:\n            action (int): Action to take (0=up, 1=down, 2=left, 3=right).\n\n        Returns:\n            state (array): Updated state after action.\n            reward (float): Reward for the action.\n            done (bool): Whether the goal was reached.\n            info (dict): Additional information.\n        \"\"\"\n        next_pos = self.agent_pos.copy()\n        if action == 0: next_pos[0] = max(self.agent_pos[0] - 1, 0)\n        if action == 1: next_pos[0] = min(self.agent_pos[0] + 1, self.size - 1)\n        if action == 2: next_pos[1] = max(self.agent_pos[1] - 1, 0)\n        if action == 3: next_pos[1] = min(self.agent_pos[1] + 1, self.size - 1)\n        if tuple(next_pos) not in self.obstacles:\n            self.prev_agent_pos = self.agent_pos.copy()\n            self.agent_pos = next_pos\n\n        done = np.array_equal(self.agent_pos, self.goal_pos)\n        dist_to_goal = np.linalg.norm(self.agent_pos - self.goal_pos)\n        prev_dist_to_goal = np.linalg.norm(self.prev_agent_pos - self.goal_pos)\n        reward = 1.0 if done else -0.01 * dist_to_goal\n        reward += 0.1 * (prev_dist_to_goal - dist_to_goal)  # Reward for getting closer to the goal\n        return self._get_state(), reward, done, {}\n\n# Define the Actor-Critic Networks\nclass ActorCNN(nn.Module):\n    def __init__(self, input_channels, action_dim):\n        \"\"\"\n        Initialize the Actor CNN for policy generation.\n\n        Args:\n            input_channels (int): Number of input channels (e.g., for CNN).\n            action_dim (int): Dimension of the action space.\n        \"\"\"\n        super(ActorCNN, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, 64, 3, 1, 1)\n        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1)\n        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)\n        self.fc1 = nn.Linear(256 * 15 * 15, 512)\n        self.fc2 = nn.Linear(512, action_dim)\n\n    def forward(self, state):\n        \"\"\"\n        Forward pass for the actor network.\n\n        Args:\n            state (Tensor): Input state tensor.\n\n        Returns:\n            Tensor: Action probabilities.\n        \"\"\"\n        x = torch.relu(self.conv1(state))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.view(x.size(0), -1)\n        x = torch.relu(self.fc1(x))\n        return torch.softmax(self.fc2(x), dim=-1)\n\nclass DDPGAgent:\n    def __init__(self, input_channels, action_dim, gamma=0.99, tau=0.005, lr=1e-4):\n        \"\"\"\n        Initialize the DDPG Agent.\n\n        Args:\n            input_channels (int): Number of input channels for the Actor network.\n            action_dim (int): Dimension of the action space.\n            gamma (float): Discount factor.\n            tau (float): Soft update parameter for target network.\n            lr (float): Learning rate for the optimizer.\n        \"\"\"\n        self.actor = ActorCNN(input_channels, action_dim).to(device)\n        self.target_actor = ActorCNN(input_channels, action_dim).to(device)\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n        self.gamma, self.tau = gamma, tau\n\n    def _soft_update(self, target, source):\n        \"\"\"\n        Perform soft update of target network parameters.\n\n        Args:\n            target (nn.Module): Target network.\n            source (nn.Module): Source network.\n        \"\"\"\n        for target_param, source_param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(self.tau * source_param.data + (1.0 - self.tau) * target_param.data)\n\n    def act(self, state, epsilon=0.1):\n        \"\"\"\n        Select an action using the actor network or explore randomly.\n\n        Args:\n            state (array): Current state of the environment.\n            epsilon (float): Exploration probability.\n\n        Returns:\n            int: Selected action.\n        \"\"\"\n        if np.random.rand() < epsilon:  # Explore with probability epsilon\n            return np.random.choice(self.actor.fc2.out_features)\n        state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)\n        with torch.no_grad():\n            action_probs = self.actor(state).cpu().numpy()[0]\n        return np.random.choice(len(action_probs), p=action_probs)\n\n    def train(self, replay_buffer, batch_size):\n        \"\"\"\n        Train the actor network using sampled transitions from the replay buffer.\n\n        Args:\n            replay_buffer (HGRReplayBuffer): Replay buffer containing transitions.\n            batch_size (int): Number of transitions to sample per training step.\n        \"\"\"\n        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n        states, rewards, dones = map(lambda x: torch.tensor(x, dtype=torch.float32).to(device), [states, rewards, dones])\n        actions = torch.tensor(actions, dtype=torch.int64).to(device)\n        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n\n        current_q = self.actor(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n        with torch.no_grad():\n            target_q = rewards + self.gamma * (1 - dones) * self.target_actor(next_states).max(dim=1)[0]\n        loss = nn.functional.mse_loss(current_q, target_q)\n\n        self.actor_optimizer.zero_grad()\n        loss.backward()\n        self.actor_optimizer.step()\n        self._soft_update(self.target_actor, self.actor)\n\n    def save_model(self, filepath):\n        \"\"\"\n        Save the actor network parameters.\n\n        Args:\n            filepath (str): Path to save the model.\n        \"\"\"\n        torch.save(self.actor.state_dict(), filepath)\n\n    def load_model(self, filepath):\n        \"\"\"\n        Load the actor network parameters.\n\n        Args:\n            filepath (str): Path to the model file.\n        \"\"\"\n        self.actor.load_state_dict(torch.load(filepath))\n\n# Define the HGR Replay Buffer\nclass HGRReplayBuffer:\n    def __init__(self, capacity, state_shape):\n        \"\"\"\n        Initialize the HGR Replay Buffer.\n\n        Args:\n            capacity (int): Maximum size of the replay buffer.\n            state_shape (tuple): Shape of the state space.\n        \"\"\"\n        self.capacity = capacity\n        self.memory = deque(maxlen=capacity)\n        self.td_errors = deque(maxlen=capacity)\n        self.state_shape = state_shape\n\n    def add(self, state, action, reward, next_state, done, td_error):\n        \"\"\"\n        Add a new transition to the replay buffer.\n\n        Args:\n            state (array): Current state.\n            action (int): Action taken.\n            reward (float): Reward received.\n            next_state (array): Next state.\n            done (bool): Whether the episode ended.\n            td_error (float): Temporal difference error.\n        \"\"\"\n        self.memory.append((state, action, reward, next_state, done))\n        self.td_errors.append(td_error)\n\n    def sample(self, batch_size):\n        \"\"\"\n        Sample a batch of transitions based on TD error priorities.\n\n        Args:\n            batch_size (int): Number of transitions to sample.\n\n        Returns:\n            tuple: Sampled transitions (states, actions, rewards, next_states, dones).\n        \"\"\"\n        probabilities = np.array(self.td_errors) / np.sum(self.td_errors)\n        indices = np.random.choice(len(self.memory), batch_size, p=probabilities)\n        batch = [self.memory[i] for i in indices]\n        return map(np.array, zip(*batch))\n\n# Compute TD Error for HGR\n\ndef compute_td_error(agent, state, action, reward, next_state, done):\n    \"\"\"\n    Compute the temporal difference (TD) error for a transition.\n\n    Args:\n        agent (DDPGAgent): The agent for which to compute TD error.\n        state (array): Current state.\n        action (int): Action taken.\n        reward (float): Reward received.\n        next_state (array): Next state.\n        done (bool): Whether the episode ended.\n\n    Returns:\n        float: TD error value.\n    \"\"\"\n    state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)\n    action = torch.tensor(action, dtype=torch.int64).to(device).unsqueeze(0)\n    reward = torch.tensor(reward, dtype=torch.float32).to(device).unsqueeze(0)\n    next_state = torch.tensor(next_state, dtype=torch.float32).to(device).unsqueeze(0)\n    done = torch.tensor(done, dtype=torch.float32).to(device).unsqueeze(0)\n\n    with torch.no_grad():\n        q_value = agent.actor(state).gather(1, action.unsqueeze(-1))\n        next_q_value = agent.target_actor(next_state).max(1)[0].unsqueeze(1)\n        target_q_value = reward + agent.gamma * (1 - done) * next_q_value\n    td_error = (q_value - target_q_value).abs()\n    return td_error.item()\n\n# Training Function with HGR\ndef train_agent_with_hgr(env, agent, episodes, max_steps, replay_buffer, batch_size):\n    \"\"\"\n    Train the agent using Hindsight Goal Ranking (HGR).\n\n    Args:\n        env (CustomMazeEnv): The maze environment.\n        agent (DDPGAgent): The DDPG agent.\n        episodes (int): Number of training episodes.\n        max_steps (int): Maximum steps per episode.\n        replay_buffer (HGRReplayBuffer): Replay buffer to store experiences.\n        batch_size (int): Batch size for training.\n    \"\"\"\n    rewards_per_episode = []\n    successes_per_episode = []\n    steps_per_episode = []\n    obstacles_per_episode = []\n\n    for ep in range(episodes):\n        state = env.reset()\n        total_reward, success, steps = 0, 0, 0\n\n        for step in range(max_steps):\n            action = agent.act(state, epsilon=max(0.1, 1 - ep / 100))\n            next_state, reward, done, _ = env.step(action)\n            td_error = compute_td_error(agent, state, action, reward, next_state, done)\n            replay_buffer.add(state, action, reward, next_state, done, td_error)\n\n            if len(replay_buffer.memory) >= batch_size:\n                agent.train(replay_buffer, batch_size)\n\n            state = next_state\n            total_reward += reward\n            steps += 1\n            if done:\n                success = 1\n                break\n\n        rewards_per_episode.append(total_reward)\n        successes_per_episode.append(success)\n        steps_per_episode.append(steps)\n        obstacles_per_episode.append(env.current_obstacles)\n\n        if ep % 10 == 0:\n            print(f\"Episode {ep}, Reward: {total_reward:.2f}, Success: {success}, Steps: {steps}, Obstacles: {env.current_obstacles}\")\n\n        if ep % 20 == 0 and ep > 0:\n            env.increase_difficulty()\n\n        if ep % 50 == 0 and ep > 0:\n            agent.save_model(f\"model_episode_{ep}.pth\")\n            save_performance_plots(rewards_per_episode, successes_per_episode, steps_per_episode, obstacles_per_episode, ep)\n\n    # Final plots\n    save_performance_plots(rewards_per_episode, successes_per_episode, steps_per_episode, obstacles_per_episode, \"final\")\n    agent.save_model(\"final_model.pth\")\n\ndef save_performance_plots(rewards, successes, steps, obstacles, suffix):\n    \"\"\"\n    Save training performance plots.\n\n    Args:\n        rewards (list): Rewards per episode.\n        successes (list): Successes per episode.\n        steps (list): Steps per episode.\n        obstacles (list): Obstacles per episode.\n        suffix (str): Suffix for the plot filenames.\n    \"\"\"\n    if not os.path.exists(\"plots\"):\n        os.makedirs(\"plots\")\n\n    plt.figure()\n    plt.plot(rewards, label=\"Rewards\")\n    plt.title(\"Rewards per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Reward\")\n    plt.legend()\n    plt.savefig(f\"plots/reward_plot_episode_{suffix}.png\")\n    plt.close()\n\n    plt.figure()\n    plt.plot(successes, label=\"Success\")\n    plt.title(\"Success per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Success\")\n    plt.legend()\n    plt.savefig(f\"plots/success_plot_episode_{suffix}.png\")\n    plt.close()\n    \n    plt.figure()\n    plt.plot(steps, label=\"Steps\")\n    plt.title(\"Steps per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Steps\")\n    plt.legend()\n    plt.savefig(f\"plots/steps_plot_episode_{suffix}.png\")\n    plt.close()\n    \n    plt.figure()\n    plt.plot(obstacles, label=\"Obstacles\")\n    plt.title(\"Obstacles per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Number of Obstacles\")\n    plt.legend()\n    plt.savefig(f\"plots/obstacles_plot_episode_{suffix}.png\")\n    plt.close()\n# Device configuration\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize the environment, replay buffer, and agent\nenv = CustomMazeEnv()\nreplay_buffer = HGRReplayBuffer(10000, env.state_dim)\nagent = DDPGAgent(env.state_dim[0], env.action_dim)\n\n# Training\ntrain_agent_with_hgr(env, agent, 500, 300, replay_buffer, 64)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:15:59.599021Z","iopub.execute_input":"2025-01-18T09:15:59.599594Z","iopub.status.idle":"2025-01-18T09:16:12.679869Z","shell.execute_reply.started":"2025-01-18T09:15:59.599538Z","shell.execute_reply":"2025-01-18T09:16:12.678477Z"}},"outputs":[{"name":"stdout","text":"Episode 0, Reward: -25.84, Success: 0, Steps: 300, Obstacles: 5\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 450\u001b[0m\n\u001b[1;32m    447\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPGAgent(env\u001b[38;5;241m.\u001b[39mstate_dim[\u001b[38;5;241m0\u001b[39m], env\u001b[38;5;241m.\u001b[39maction_dim)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m \u001b[43mtrain_agent_with_hgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 360\u001b[0m, in \u001b[0;36mtrain_agent_with_hgr\u001b[0;34m(env, agent, episodes, max_steps, replay_buffer, batch_size)\u001b[0m\n\u001b[1;32m    358\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ep \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m    359\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m--> 360\u001b[0m td_error \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_td_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39madd(state, action, reward, next_state, done, td_error)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n","Cell \u001b[0;32mIn[2], line 322\u001b[0m, in \u001b[0;36mcompute_td_error\u001b[0;34m(agent, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_td_error\u001b[39m(agent, state, action, reward, next_state, done):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Compute the temporal difference (TD) error for a transition.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m        float: TD error value.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    323\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(action, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    324\u001b[0m     reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"def test_model_with_multiple_runs_and_save(model_path, env_size=15, obstacles=50, max_steps=800, visualize_every=10, num_runs=3):\n    \"\"\"\n    Test the model on multiple runs with random start and goal positions, save all results in a structured folder.\n\n    Args:\n        model_path (str): Path to the saved model.\n        env_size (int): Size of the maze (default: 15x15).\n        obstacles (int): Maximum number of obstacles in the maze.\n        max_steps (int): Maximum number of steps allowed per run.\n        visualize_every (int): Visualize the maze every n steps.\n        num_runs (int): Number of runs with different start and goal positions.\n\n    Returns:\n        None\n    \"\"\"\n    import torch\n    import os\n    import shutil\n    import numpy as np\n    from matplotlib import pyplot as plt\n\n    # Initialize directories for storing results\n    base_dir = \"benchmark/Train_with_changing_position_test_different\"\n    os.makedirs(base_dir, exist_ok=True)\n\n    # Create subdirectories for each run\n    run_dirs = [os.path.join(base_dir, f\"Run_{i+1}\") for i in range(num_runs)]\n    for run_dir in run_dirs:\n        os.makedirs(run_dir, exist_ok=True)\n\n    # Load the model\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    agent = DDPGAgent(input_channels=3, action_dim=4)\n    try:\n        agent.actor.load_state_dict(torch.load(model_path, map_location=device))\n        agent.actor.eval()  # Set the model to evaluation mode\n        print(f\"Model loaded successfully from {model_path}\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return\n\n    all_rewards = []  # To store rewards for each run\n\n    for run_idx, run_dir in enumerate(run_dirs):\n        print(f\"\\nStarting run {run_idx + 1}/{num_runs}\")\n\n        # Initialize the environment\n        env = CustomMazeEnv(size=env_size, initial_obstacles=obstacles, max_obstacles=obstacles)\n        env.reset()\n\n        # Visualization function to save maze images\n        def save_maze_image(env, title, step):\n            plt.figure(figsize=(8, 8))\n            maze = np.zeros((env.size, env.size))\n            for obs in env.obstacles:\n                maze[obs[0], obs[1]] = 1  # Obstacles\n            plt.imshow(maze, cmap=\"binary\")\n            plt.scatter(env.agent_pos[1], env.agent_pos[0], c=\"blue\", label=\"Agent\")\n            plt.scatter(env.goal_pos[1], env.goal_pos[0], c=\"green\", label=\"Goal\")\n            plt.title(title)\n            plt.legend(loc=\"upper right\")\n            file_path = os.path.join(run_dir, f\"Step_{step}.png\")\n            plt.savefig(file_path)\n            plt.close()\n\n        # Save the initial maze image\n        save_maze_image(env, title=f\"Run {run_idx + 1} - Initial Maze\", step=\"initial\")\n\n        # Test the agent on the environment\n        state = env._get_state()\n        total_reward = 0\n        rewards = []\n\n        for step in range(max_steps):\n            action = agent.act(state, epsilon=0)  # No exploration during testing\n            next_state, reward, done, _ = env.step(action)\n            total_reward += reward\n            rewards.append(total_reward)\n            state = next_state\n\n            # Save maze visualization every `visualize_every` steps or if the goal is reached\n            if step % visualize_every == 0 or done:\n                save_maze_image(env, title=f\"Run {run_idx + 1} - Step {step}\", step=step)\n\n            if done:\n                print(f\"Goal reached in {step + 1} steps with total reward {total_reward:.2f}\")\n                break\n\n        all_rewards.append(rewards)\n\n    # Save performance plots for all runs\n    performance_plot_path = os.path.join(base_dir, \"Performance_Plot.png\")\n    plt.figure(figsize=(10, 5))\n    for i, rewards in enumerate(all_rewards):\n        plt.plot(rewards, label=f\"Run {i + 1}\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Cumulative Reward\")\n    plt.title(\"Agent Performance Across Runs\")\n    plt.legend()\n    plt.grid()\n    plt.savefig(performance_plot_path)\n    plt.close()\n\n    # Compress the benchmark directory into a zip file for easier download\n    shutil.make_archive(\"benchmark\", 'zip', base_dir)\n    print(\"Results saved and compressed into benchmark.zip.\")\n\n\ntest_model_with_multiple_runs_and_save(\n    model_path=\"/kaggle/input/test1234/final_model (1).pth\",\n    env_size=15,\n    obstacles=50,\n    max_steps=800,\n    visualize_every=10,\n    num_runs=3\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:16:14.960709Z","iopub.execute_input":"2025-01-18T09:16:14.961447Z","iopub.status.idle":"2025-01-18T09:16:30.887980Z","shell.execute_reply.started":"2025-01-18T09:16:14.961396Z","shell.execute_reply":"2025-01-18T09:16:30.886857Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3810554987.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  agent.actor.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully from /kaggle/input/test1234/final_model (1).pth\n\nStarting run 1/3\nGoal reached in 68 steps with total reward -2.33\n\nStarting run 2/3\nGoal reached in 338 steps with total reward -23.40\n\nStarting run 3/3\nGoal reached in 10 steps with total reward 1.03\nResults saved and compressed into benchmark.zip.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport matplotlib.pyplot as plt\nimport os\n\n# Define Maze Environment with Curriculum Learning\nclass CustomMazeEnv:\n    def __init__(self, size=15, initial_obstacles=5, max_obstacles=50):\n        self.size = size\n        self.state_dim = (3, size, size)  # Channels for agent, goal, obstacles\n        self.action_dim = 4  # Actions: 0 = up, 1 = down, 2 = left, 3 = right\n        self.initial_obstacles = initial_obstacles\n        self.max_obstacles = max_obstacles\n        self.current_obstacles = initial_obstacles\n\n    def reset(self):\n        \"\"\"\n        Reset the maze environment to a new configuration with random agent and goal positions.\n        \n        Returns:\n            state (np.ndarray): Initial state of the environment.\n        \"\"\"\n        while True:\n            self.agent_pos = np.random.randint(0, self.size, size=2)\n            self.goals = [tuple(np.random.randint(0, self.size, size=2))]\n            self.current_goal_index = 0\n\n            if not any(np.array_equal(self.agent_pos, goal) for goal in self.goals):\n                self._generate_maze()\n                if self._is_path_possible():\n                    break\n\n        return self._get_state()\n\n    def increase_difficulty(self):\n        self.current_obstacles = min(self.current_obstacles + 1, self.max_obstacles)\n\n    def _generate_maze(self):\n        while True:\n            self.obstacles = []\n            maze = np.zeros((self.size, self.size), dtype=int)\n            maze[self.agent_pos[0], self.agent_pos[1]] = 1\n            for goal in self.goals:\n                maze[goal[0], goal[1]] = 1\n\n            for i in range(self.size):\n                for j in range(self.size):\n                    if maze[i, j] == 0 and np.random.rand() < self.current_obstacles / (self.size ** 2):\n                        self.obstacles.append((i, j))\n\n            if self._is_path_possible():\n                break\n\n    def _is_path_possible(self):\n        def dfs(x, y, visited):\n            if (x, y) == tuple(self.goals[self.current_goal_index]):\n                return True\n            visited.add((x, y))\n            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < self.size and 0 <= ny < self.size and (nx, ny) not in visited and (nx, ny) not in self.obstacles:\n                    if dfs(nx, ny, visited):\n                        return True\n            return False\n\n        return dfs(self.agent_pos[0], self.agent_pos[1], set())\n\n    def _get_state(self):\n        state = np.zeros(self.state_dim, dtype=np.float32)\n        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0\n        for i, goal in enumerate(self.goals):\n            state[1 + i % 2, goal[0], goal[1]] = 1.0\n        for obs in self.obstacles:\n            state[2, obs[0], obs[1]] = 1.0\n        return state\n\n    def step(self, action):\n        next_pos = self.agent_pos.copy()\n        if action == 0: next_pos[0] = max(self.agent_pos[0] - 1, 0)\n        if action == 1: next_pos[0] = min(self.agent_pos[0] + 1, self.size - 1)\n        if action == 2: next_pos[1] = max(self.agent_pos[1] - 1, 0)\n        if action == 3: next_pos[1] = min(self.agent_pos[1] + 1, self.size - 1)\n        if tuple(next_pos) not in self.obstacles:\n            self.agent_pos = next_pos\n        done = np.array_equal(self.agent_pos, self.goals[self.current_goal_index])\n        reward = 1.0 if done else -0.01 * np.linalg.norm(self.agent_pos - self.goals[self.current_goal_index])\n\n        if done:\n            self.current_goal_index += 1\n            if self.current_goal_index >= len(self.goals):\n                done = True\n            else:\n                done = False\n        return self._get_state(), reward, done, {}\n\nclass MultiLevelMazeEnv(CustomMazeEnv):\n    def __init__(self, size=15, initial_obstacles=5, max_obstacles=50, levels=3):\n        self.levels = levels\n        super().__init__(size, initial_obstacles, max_obstacles)\n        self.current_level = 0\n\n    def reset(self):\n        self.current_level = 0\n        self.current_goal_index = 0  # Initialize goal index\n        return self._reset_level()\n\n    def _reset_level(self):\n        while True:\n            self.agent_pos = np.random.randint(0, self.size, size=2)\n            self.goals = [tuple(np.random.randint(0, self.size, size=2))]\n            self.current_goal_index = 0  # Initialize goal index\n            if not any(np.array_equal(self.agent_pos, goal) for goal in self.goals):\n                self._generate_maze()\n                if self._is_path_possible():\n                    break\n\n        return self._get_state()\n\n    def step(self, action):\n        state, reward, done, info = super().step(action)\n        if done:\n            self.current_level += 1\n            if self.current_level >= self.levels:\n                done = True\n            else:\n                state = self._reset_level()\n                done = False\n        return state, reward, done, info\n\n\ndef benchmark_multi_level_with_model(env, agent, model_path, episodes, max_steps):\n    agent.load_model(model_path)\n    print(f\"Model loaded from {model_path}\")\n\n    rewards_per_episode = []\n    successes_per_episode = []\n    levels_completed = []\n\n    for ep in range(episodes):\n        state = env.reset()\n        total_reward, success, steps = 0, 0, 0\n        completed_levels = 0\n\n        for step in range(max_steps):\n            action = agent.act(state)\n            next_state, reward, done, _ = env.step(action)\n            state = next_state\n            total_reward += reward\n            steps += 1\n\n            if done:\n                if env.current_level >= env.levels:\n                    success = 1\n                completed_levels = env.current_level\n                break\n\n        rewards_per_episode.append(total_reward)\n        successes_per_episode.append(success)\n        levels_completed.append(completed_levels)\n\n        print(f\"Episode {ep}, Reward: {total_reward:.2f}, Success: {success}, Levels Completed: {completed_levels}\")\n\n    # Plot results\n    plot_multi_level_results(rewards_per_episode, successes_per_episode, levels_completed)\n\ndef plot_multi_level_results(rewards, successes, levels_completed):\n    if not os.path.exists(\"plots\"):\n        os.makedirs(\"plots\")\n\n    plt.figure()\n    plt.plot(rewards, label=\"Rewards\")\n    plt.title(\"Rewards per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Total Reward\")\n    plt.legend()\n    plt.savefig(\"plots/multi_level_rewards_plot.png\")\n    plt.close()\n\n    plt.figure()\n    plt.plot(successes, label=\"Success Rate\")\n    plt.title(\"Success per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Success (1 = Success, 0 = Failure)\")\n    plt.legend()\n    plt.savefig(\"plots/multi_level_successes_plot.png\")\n    plt.close()\n\n    plt.figure()\n    plt.plot(levels_completed, label=\"Levels Completed\")\n    plt.title(\"Levels Completed per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Levels Completed\")\n    plt.legend()\n    plt.savefig(\"plots/levels_completed_plot.png\")\n    plt.close()\n\n    print(\"Plots saved in the 'plots' directory.\")\n\n# Device configuration\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize environment and agent\nmulti_level_env = MultiLevelMazeEnv(size=15, initial_obstacles=5, max_obstacles=50, levels=3)\nagent = DDPGAgent(input_channels=3, action_dim=4)\n\n# Path to the saved model\nmodel_path = \"/kaggle/input/test1234/final_model (1).pth\"  # Update with the correct path to your model file\n\n# Run the benchmark\nbenchmark_multi_level_with_model(multi_level_env, agent, model_path, episodes=300, max_steps=400)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def benchmark_multi_objective_with_model(env, agent, model_path, episodes=10, max_steps=500):\n    \"\"\"\n    Test the agent on a multi-objective maze environment with a pretrained model.\n    Saves reward and goal completion plots.\n\n    Args:\n        env (MultiObjectiveMazeEnv): Multi-objective maze environment.\n        agent (DDPGAgent): Agent to be tested.\n        model_path (str): Path to the pretrained model file.\n        episodes (int): Number of episodes to test.\n        max_steps (int): Maximum steps per episode.\n\n    Returns:\n        None\n    \"\"\"\n    agent.load_model(model_path)\n    print(f\"Model loaded successfully from {model_path}\")\n\n    rewards_per_episode = []\n    goals_reached_per_episode = []\n\n    for ep in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        goals_reached = 0\n\n        for step in range(max_steps):\n            action = agent.act(state, epsilon=0)  # No exploration during testing\n            next_state, reward, done, _ = env.step(action)\n            state = next_state\n\n            total_reward += reward\n            if done:\n                goals_reached = env.current_goal_index  # Number of goals reached\n                break\n\n        rewards_per_episode.append(total_reward)\n        goals_reached_per_episode.append(goals_reached)\n\n        print(f\"Episode {ep + 1}/{episodes}: Total Reward = {total_reward:.2f}, Goals Reached = {goals_reached}\")\n\n    # Plot results\n    plot_multi_objective_results(rewards_per_episode, goals_reached_per_episode)\n\n\ndef plot_multi_objective_results(rewards, goals_reached):\n    \"\"\"\n    Plot rewards and goals reached during testing.\n\n    Args:\n        rewards (list): Total rewards per episode.\n        goals_reached (list): Number of goals reached per episode.\n\n    Returns:\n        None\n    \"\"\"\n    if not os.path.exists(\"plots\"):\n        os.makedirs(\"plots\")\n\n    # Plot rewards per episode\n    plt.figure()\n    plt.plot(rewards, label=\"Rewards\")\n    plt.title(\"Rewards per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Total Reward\")\n    plt.legend()\n    plt.savefig(\"plots/multi_objective_rewards_plot_HER.png\")\n    plt.close()\n\n    # Plot goals reached per episode\n    plt.figure()\n    plt.plot(goals_reached, label=\"Goals Reached\")\n    plt.title(\"Goals Reached per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Number of Goals Reached\")\n    plt.legend()\n    plt.savefig(\"plots/multi_objective_goals_reached_plot_HER.png\")\n    plt.close()\n\n    print(\"Plots saved in the 'plots' directory.\")\n\n# Main setup\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmulti_objective_env = MultiObjectiveMazeEnv(size=15, initial_obstacles=5, max_obstacles=50, num_goals=3)\nagent = DDPGAgent(input_channels=3, action_dim=4)\n\nmodel_path = \"/kaggle/input/test1234/final_model (1).pth\"\nbenchmark_multi_objective_with_model(multi_objective_env, agent, model_path, episodes=10, max_steps=500)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom collections import deque\nimport random\nimport matplotlib.pyplot as plt\nimport os\n\n# Actor Network for SAC\nclass ActorSAC(nn.Module):\n    \"\"\"\n    Actor network for the Soft Actor-Critic (SAC) algorithm.\n\n    Attributes:\n        conv1, conv2, conv3: Convolutional layers for processing the maze state.\n        fc1, fc2: Fully connected layers for generating action probabilities.\n    \"\"\"\n    def __init__(self, input_channels, action_dim, maze_size):\n        super(ActorSAC, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, 64, 3, 1, 1)\n        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1)\n        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)\n        self.fc1 = nn.Linear(256 * maze_size * maze_size, 512)\n        self.fc2 = nn.Linear(512, action_dim)\n\n    def forward(self, state):\n        \"\"\"\n        Forward pass for the actor network.\n        Args:\n            state: Input maze state.\n        Returns:\n            Softmax probabilities over actions.\n        \"\"\"\n        x = torch.relu(self.conv1(state))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.view(x.size(0), -1)\n        x = torch.relu(self.fc1(x))\n        return torch.softmax(self.fc2(x), dim=-1)\n\n# Critic Network for SAC\nclass CriticSAC(nn.Module):\n    \"\"\"\n    Critic network for estimating Q-values in SAC.\n\n    Attributes:\n        conv1, conv2, conv3: Convolutional layers for processing the maze state.\n        fc1, fc2: Fully connected layers for Q-value estimation.\n    \"\"\"\n    def __init__(self, input_channels, action_dim, maze_size):\n        super(CriticSAC, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, 64, 3, 1, 1)\n        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1)\n        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)\n        self.fc1 = nn.Linear(256 * maze_size * maze_size + action_dim, 512)\n        self.fc2 = nn.Linear(512, 1)\n\n    def forward(self, state, action):\n        \"\"\"\n        Forward pass for the critic network.\n        Args:\n            state: Input maze state.\n            action: Action taken by the agent.\n        Returns:\n            Q-value for the state-action pair.\n        \"\"\"\n        x = torch.relu(self.conv1(state))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.view(x.size(0), -1)\n        x = torch.cat([x, action], dim=-1)  # Combine state and action\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\n# SAC Agent\nclass SACAgent:\n    \"\"\"\n    Soft Actor-Critic (SAC) agent for maze-solving tasks.\n\n    Attributes:\n        actor: The policy network.\n        critic1, critic2: Q-value estimators.\n        target_critic1, target_critic2: Target networks for stability.\n        optimizers: Optimizers for actor and critic networks.\n        gamma: Discount factor for rewards.\n        tau: Soft update factor for target networks.\n        alpha: Entropy coefficient for exploration.\n    \"\"\"\n    def __init__(self, input_channels, action_dim, maze_size, lr=3e-4, gamma=0.99, tau=0.005, alpha=0.2):\n        self.actor = ActorSAC(input_channels, action_dim, maze_size).to(device)\n        self.critic1 = CriticSAC(input_channels, action_dim, maze_size).to(device)\n        self.critic2 = CriticSAC(input_channels, action_dim, maze_size).to(device)\n        self.target_critic1 = CriticSAC(input_channels, action_dim, maze_size).to(device)\n        self.target_critic2 = CriticSAC(input_channels, action_dim, maze_size).to(device)\n\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=lr)\n        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=lr)\n\n        self.gamma = gamma\n        self.tau = tau\n        self.alpha = alpha\n\n        self.update_targets()  # Initialize target networks\n\n    def update_targets(self):\n        \"\"\"\n        Soft update the target networks using the online networks.\n        \"\"\"\n        for target_param, param in zip(self.target_critic1.parameters(), self.critic1.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n        for target_param, param in zip(self.target_critic2.parameters(), self.critic2.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n    def act(self, state):\n        \"\"\"\n        Select an action based on the current policy.\n        Args:\n            state: Current state.\n        Returns:\n            Chosen action.\n        \"\"\"\n        state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)\n        with torch.no_grad():\n            action_probs = self.actor(state).cpu().numpy()[0]\n        action = np.random.choice(len(action_probs), p=action_probs)\n        return action\n\n    def train(self, replay_buffer, batch_size):\n        \"\"\"\n        Train the agent using the replay buffer.\n        Args:\n            replay_buffer: The replay buffer storing experiences.\n            batch_size: Number of samples for each training step.\n        \"\"\"\n        # Sample a batch from the replay buffer\n        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n        states = torch.tensor(states, dtype=torch.float32).to(device)\n        actions = torch.tensor(actions, dtype=torch.long).to(device)\n        rewards = torch.tensor(rewards, dtype=torch.float32).to(device).unsqueeze(-1)\n        next_states = torch.tensor(next_states, dtype=torch.float32).to(device)\n        dones = torch.tensor(dones, dtype=torch.float32).to(device).unsqueeze(-1)\n\n        # Compute target Q values\n        with torch.no_grad():\n            next_action_probs = self.actor(next_states)\n            next_q1 = self.target_critic1(next_states, next_action_probs)\n            next_q2 = self.target_critic2(next_states, next_action_probs)\n            next_q = torch.min(next_q1, next_q2) - self.alpha * torch.log(next_action_probs + 1e-10).sum(dim=-1, keepdim=True)\n            target_q = rewards + self.gamma * (1 - dones) * next_q\n\n        # Update critics\n        current_q1 = self.critic1(states, nn.functional.one_hot(actions, num_classes=4).float().to(device))\n        current_q2 = self.critic2(states, nn.functional.one_hot(actions, num_classes=4).float().to(device))\n        critic1_loss = nn.functional.mse_loss(current_q1, target_q)\n        critic2_loss = nn.functional.mse_loss(current_q2, target_q)\n        self.critic1_optimizer.zero_grad()\n        critic1_loss.backward()\n        self.critic1_optimizer.step()\n        self.critic2_optimizer.zero_grad()\n        critic2_loss.backward()\n        self.critic2_optimizer.step()\n\n        # Update actor\n        action_probs = self.actor(states)\n        q1 = self.critic1(states, action_probs)\n        q2 = self.critic2(states, action_probs)\n        actor_loss = (self.alpha * torch.log(action_probs + 1e-10).sum(dim=-1) - torch.min(q1, q2)).mean()\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # Soft update targets\n        self.update_targets()\n\n# Replay Buffer\nclass ReplayBuffer:\n    \"\"\"\n    Replay buffer for storing and sampling experiences.\n\n    Attributes:\n        capacity: Maximum number of experiences to store.\n        buffer: Deque to store experiences.\n    \"\"\"\n    def __init__(self, capacity, state_dim):\n        self.capacity = capacity\n        self.buffer = deque(maxlen=capacity)\n\n    def add(self, state, action, reward, next_state, done):\n        \"\"\"\n        Add a transition to the buffer.\n        \"\"\"\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        \"\"\"\n        Sample a batch of transitions from the buffer.\n        \"\"\"\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n        return states, actions, rewards, next_states, dones\n\n    def __len__(self):\n        return len(self.buffer)\n\n# Training Function\ndef train_sac_with_metrics(env, agent, replay_buffer, episodes, max_steps, batch_size, output_dir):\n    \"\"\"\n    Train the SAC agent and log metrics.\n\n    Args:\n        env: The environment.\n        agent: The SAC agent.\n        replay_buffer: Replay buffer storing experiences.\n        episodes: Number of training episodes.\n        max_steps: Maximum steps per episode.\n        batch_size: Number of samples per training step.\n        output_dir: Directory to save metrics.\n    \"\"\"\n    rewards_per_episode = []\n    successes_per_episode = []\n\n    for episode in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        success = 0\n\n        for step in range(max_steps):\n            action = agent.act(state)  # Select action\n            next_state, reward, done, _ = env.step(action)  # Execute action\n            replay_buffer.add(state, action, reward, next_state, done)  # Store transition\n\n            if len(replay_buffer) > batch_size:\n                agent.train(replay_buffer, batch_size)  # Train agent\n\n            state = next_state\n            total_reward += reward\n\n            if done:\n                success = 1  # Goal reached\n                break\n\n        # Log metrics\n        rewards_per_episode.append(total_reward)\n        successes_per_episode.append(success)\n\n        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Success: {success}\")\n\n        if (episode + 1) % 20 == 0:\n            env.increase_difficulty()  # Curriculum learning\n\n    # Save metrics as plots\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    plt.figure()\n    plt.plot(rewards_per_episode, label=\"Reward\")\n    plt.title(\"Rewards per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Total Reward\")\n    plt.legend()\n    plt.savefig(os.path.join(output_dir, \"reward_plot.png\"))\n    plt.close()\n\n    plt.figure()\n    plt.plot(successes_per_episode, label=\"Success\")\n    plt.title(\"Success Rate per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Success (1/0)\")\n    plt.legend()\n    plt.savefig(os.path.join(output_dir, \"success_plot.png\"))\n    plt.close()\n\n# Main Script\nif __name__ == \"__main__\":\n    env_size = 15\n    obstacles = 5\n    max_steps = 300\n    episodes = 200\n    batch_size = 64\n    output_dir = \"sac_results\"\n\n    env = CustomMazeEnv(size=env_size, initial_obstacles=obstacles, max_obstacles=50)\n    replay_buffer = ReplayBuffer(10000, env.state_dim)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    sac_agent = SACAgent(input_channels=3, action_dim=4, maze_size=env_size)\n\n    train_sac_with_metrics(env, sac_agent, replay_buffer, episodes, max_steps, batch_size, output_dir)\n\n    # Save the SAC model\n    torch.save(sac_agent.actor.state_dict(), \"sac_model.pth\")\n    print(\"SAC model saved as sac_model.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\nclass CustomMazeEnv:\n    \"\"\"\n    Base class for a customizable maze environment.\n\n    Attributes:\n        size (int): Size of the maze (NxN grid).\n        state_dim (tuple): Shape of the state (channels, size, size).\n        action_dim (int): Number of possible actions (4: up, down, left, right).\n        initial_obstacles (int): Initial number of obstacles in the maze.\n        max_obstacles (int): Maximum number of obstacles allowed in the maze.\n        current_obstacles (int): Current number of obstacles in the maze.\n    \"\"\"\n\n    def __init__(self, size=15, initial_obstacles=5, max_obstacles=50):\n        self.size = size\n        self.state_dim = (3, size, size)  # Channels for agent, goal, and obstacles\n        self.action_dim = 4  # Actions: 0 = up, 1 = down, 2 = left, 3 = right\n        self.initial_obstacles = initial_obstacles\n        self.max_obstacles = max_obstacles\n        self.current_obstacles = initial_obstacles\n\n    def _generate_maze(self):\n        \"\"\"\n        Generates a maze with obstacles while ensuring a valid path exists\n        between the agent's position and the current goal.\n        \"\"\"\n        self.obstacles = []\n        while True:\n            maze = np.zeros((self.size, self.size), dtype=int)\n            maze[self.agent_pos[0], self.agent_pos[1]] = 1  # Mark agent position\n            for goal in self.goals:\n                maze[goal[0], goal[1]] = 1  # Mark goal positions\n\n            # Place obstacles randomly based on the obstacle density\n            for i in range(self.size):\n                for j in range(self.size):\n                    if maze[i, j] == 0 and np.random.rand() < self.current_obstacles / (self.size ** 2):\n                        self.obstacles.append((i, j))\n\n            if self._is_path_possible():\n                break  # Ensure a valid path exists before proceeding\n\n    def _is_path_possible(self):\n        \"\"\"\n        Checks if there is a valid path from the agent's position to the current goal.\n        Uses depth-first search (DFS) to verify connectivity.\n        \"\"\"\n        def dfs(x, y, visited):\n            if (x, y) == tuple(self.goals[self.current_goal_index]):\n                return True\n            visited.add((x, y))\n            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n                nx, ny = x + dx, y + dy\n                if 0 <= nx < self.size and 0 <= ny < self.size and (nx, ny) not in visited and (nx, ny) not in self.obstacles:\n                    if dfs(nx, ny, visited):\n                        return True\n            return False\n\n        return dfs(self.agent_pos[0], self.agent_pos[1], set())\n\n    def _get_state(self):\n        \"\"\"\n        Constructs the current state representation as a 3D tensor.\n\n        Returns:\n            numpy.ndarray: 3D state representation with separate channels for\n            agent position, goals, and obstacles.\n        \"\"\"\n        state = np.zeros(self.state_dim, dtype=np.float32)\n        state[0, self.agent_pos[0], self.agent_pos[1]] = 1.0  # Agent position\n        for goal in self.goals:\n            state[1, goal[0], goal[1]] = 1.0  # Goals\n        for obs in self.obstacles:\n            state[2, obs[0], obs[1]] = 1.0  # Obstacles\n        return state\n\n\nclass MultiObjectiveMazeEnv(CustomMazeEnv):\n    \"\"\"\n    Maze environment with multiple sequential goals.\n\n    Attributes:\n        num_goals (int): Number of goals in the maze.\n        goals (list): List of goal positions in the maze.\n        current_goal_index (int): Index of the current goal the agent is pursuing.\n    \"\"\"\n\n    def __init__(self, size=15, initial_obstacles=5, max_obstacles=50, num_goals=3):\n        super().__init__(size, initial_obstacles, max_obstacles)\n        self.num_goals = num_goals\n        self.goals = []\n        self.current_goal_index = 0\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Resets the environment, reinitializing the agent, goals, and maze.\n        Ensures the agent does not start on a goal position.\n        \"\"\"\n        self.agent_pos = np.random.randint(0, self.size, size=2)\n        self.goals = [tuple(np.random.randint(0, self.size, size=2)) for _ in range(self.num_goals)]\n\n        while any(np.array_equal(self.agent_pos, goal) for goal in self.goals):\n            self.agent_pos = np.random.randint(0, self.size, size=2)\n\n        self.current_goal_index = 0\n        self._generate_maze()\n        return self._get_state()\n\n    def step(self, action):\n        \"\"\"\n        Executes an action in the environment, updating the agent's position.\n\n        Args:\n            action (int): The action to perform (0 = up, 1 = down, 2 = left, 3 = right).\n\n        Returns:\n            tuple: (state, reward, done, info)\n            - state: The next state after the action.\n            - reward: Reward received for the action.\n            - done: Whether the current episode is complete.\n            - info: Additional info (empty for now).\n        \"\"\"\n        next_pos = self.agent_pos.copy()\n        if action == 0: next_pos[0] = max(self.agent_pos[0] - 1, 0)  # Move up\n        if action == 1: next_pos[0] = min(self.agent_pos[0] + 1, self.size - 1)  # Move down\n        if action == 2: next_pos[1] = max(self.agent_pos[1] - 1, 0)  # Move left\n        if action == 3: next_pos[1] = min(self.agent_pos[1] + 1, self.size - 1)  # Move right\n\n        if tuple(next_pos) not in self.obstacles:\n            self.agent_pos = next_pos\n\n        done = np.array_equal(self.agent_pos, self.goals[self.current_goal_index])\n        reward = 1.0 if done else -0.01 * np.linalg.norm(self.agent_pos - self.goals[self.current_goal_index])\n\n        if done:\n            self.current_goal_index += 1\n            done = self.current_goal_index >= len(self.goals)\n\n        return self._get_state(), reward, done, {}\n\n\ndef benchmark_multi_objective_with_model(env, agent, model_path, episodes=10, max_steps=500):\n    \"\"\"\n    Benchmarks a pretrained SAC agent on a multi-objective maze environment.\n\n    Args:\n        env (MultiObjectiveMazeEnv): The multi-objective maze environment.\n        agent (SACAgent): The SAC agent to be benchmarked.\n        model_path (str): Path to the pretrained model file.\n        episodes (int): Number of episodes to run the benchmark.\n        max_steps (int): Maximum steps per episode.\n\n    Returns:\n        None\n    \"\"\"\n    agent.actor.load_state_dict(torch.load(model_path, map_location=device))\n    print(f\"Model loaded successfully from {model_path}\")\n\n    rewards_per_episode = []\n    goals_reached_per_episode = []\n\n    for ep in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        goals_reached = 0\n\n        for step in range(max_steps):\n            action = agent.act(state)\n            next_state, reward, done, _ = env.step(action)\n            state = next_state\n\n            total_reward += reward\n            if done:\n                goals_reached = env.current_goal_index\n                break\n\n        rewards_per_episode.append(total_reward)\n        goals_reached_per_episode.append(goals_reached)\n\n        print(f\"Episode {ep + 1}/{episodes}: Total Reward = {total_reward:.2f}, Goals Reached = {goals_reached}\")\n\n    plot_multi_objective_results(rewards_per_episode, goals_reached_per_episode)\n\n\ndef plot_multi_objective_results(rewards, goals_reached):\n    \"\"\"\n    Saves plots for rewards and goals reached per episode.\n\n    Args:\n        rewards (list): Total rewards per episode.\n        goals_reached (list): Number of goals reached per episode.\n    \"\"\"\n    if not os.path.exists(\"plots\"):\n        os.makedirs(\"plots\")\n\n    plt.figure()\n    plt.plot(rewards, label=\"Rewards\")\n    plt.title(\"Rewards per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Total Reward\")\n    plt.legend()\n    plt.savefig(\"plots/sac_multi_objective_rewards_plot.png\")\n    plt.close()\n\n    plt.figure()\n    plt.plot(goals_reached, label=\"Goals Reached\")\n    plt.title(\"Goals Reached per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Number of Goals Reached\")\n    plt.legend()\n    plt.savefig(\"plots/sac_multi_objective_goals_reached_plot.png\")\n    plt.close()\n\n    print(\"Plots saved in the 'plots' directory.\")\n\n\n\n\nmulti_objective_env = MultiObjectiveMazeEnv(size=15, initial_obstacles=5, max_obstacles=50, num_goals=3)\nsac_agent = SACAgent(input_channels=3, action_dim=4, maze_size=15)\nbenchmark_multi_objective_with_model(multi_objective_env, sac_agent, \"sac_model.pth\", episodes=10, max_steps=500)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def benchmark_multi_level_with_model(env, agent, model_path, episodes, max_steps):\n    \"\"\"\n    Benchmarks a pretrained SAC agent on a multi-level maze environment.\n\n    Args:\n        env (MultiLevelMazeEnv): The multi-level maze environment.\n        agent (SACAgent): The SAC agent to be benchmarked.\n        model_path (str): Path to the pretrained model file.\n        episodes (int): Number of episodes to run the benchmark.\n        max_steps (int): Maximum steps allowed per episode.\n\n    Returns:\n        None\n    \"\"\"\n    # Load the pretrained model\n    agent.actor.load_state_dict(torch.load(model_path, map_location=device))\n    print(f\"Model loaded from {model_path}\")\n\n    # Lists to store metrics for each episode\n    rewards_per_episode = []  # Total rewards per episode\n    successes_per_episode = []  # Success (1/0) for each episode\n    levels_completed = []  # Number of levels completed per episode\n\n    for ep in range(episodes):\n        # Reset the environment at the start of each episode\n        state = env.reset()\n        total_reward, success, completed_levels = 0, 0, 0\n\n        for step in range(max_steps):\n            # Select and execute an action\n            action = agent.act(state)\n            next_state, reward, done, _ = env.step(action)\n            state = next_state\n\n            total_reward += reward  # Accumulate rewards\n\n            if done:\n                # Check if the final level is reached\n                success = 1 if env.current_level >= env.levels else 0\n                completed_levels = env.current_level  # Record the levels completed\n                break\n\n        # Record episode metrics\n        rewards_per_episode.append(total_reward)\n        successes_per_episode.append(success)\n        levels_completed.append(completed_levels)\n\n        # Print episode summary\n        print(f\"Episode {ep + 1}/{episodes}: Reward = {total_reward:.2f}, \"\n              f\"Success = {success}, Levels = {completed_levels}\")\n\n    # Generate plots for results\n    plot_multi_level_results(rewards_per_episode, successes_per_episode, levels_completed)\n\n\ndef plot_multi_level_results(rewards, successes, levels):\n    \"\"\"\n    Generates and saves plots for benchmarking results in a multi-level environment.\n\n    Args:\n        rewards (list): Total rewards per episode.\n        successes (list): Success (1/0) for each episode.\n        levels (list): Number of levels completed per episode.\n\n    Returns:\n        None\n    \"\"\"\n    if not os.path.exists(\"plots\"):\n        os.makedirs(\"plots\")  # Ensure the plots directory exists\n\n    # Plot rewards per episode\n    plt.figure()\n    plt.plot(rewards, label=\"Rewards\")\n    plt.title(\"Rewards per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Total Reward\")\n    plt.legend()\n    plt.savefig(\"plots/multi_level_rewards_plot.png\")\n    plt.close()\n\n    # Plot success rate per episode\n    plt.figure()\n    plt.plot(successes, label=\"Success\")\n    plt.title(\"Success Rate per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Success (1/0)\")\n    plt.legend()\n    plt.savefig(\"plots/multi_level_success_plot.png\")\n    plt.close()\n\n    # Plot levels completed per episode\n    plt.figure()\n    plt.plot(levels, label=\"Levels Completed\")\n    plt.title(\"Levels Completed per Episode\")\n    plt.xlabel(\"Episode\")\n    plt.ylabel(\"Number of Levels Completed\")\n    plt.legend()\n    plt.savefig(\"plots/multi_level_levels_completed_plot.png\")\n    plt.close()\n\n    print(\"Plots saved in the 'plots' directory.\")\n\n\n\nmulti_level_env = MultiLevelMazeEnv(size=15, initial_obstacles=5, max_obstacles=50, levels=3)\n\nbenchmark_multi_level_with_model(multi_level_env, sac_agent, \"sac_model.pth\", episodes=300, max_steps=400)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_sac_model_with_multiple_runs_and_save(model_path, env_size=15, obstacles=50, max_steps=800, num_runs=3):\n    \"\"\"\n    Tests a pretrained SAC agent across multiple runs and saves the performance results.\n\n    Args:\n        model_path (str): Path to the pretrained SAC model file.\n        env_size (int): Size of the maze environment (NxN grid).\n        obstacles (int): Number of obstacles in the maze.\n        max_steps (int): Maximum number of steps allowed per run.\n        num_runs (int): Number of independent runs to perform.\n\n    Returns:\n        None. Saves performance plots and results to the `benchmark` directory.\n    \"\"\"\n    # Create base directory to save results\n    base_dir = \"benchmark/sac_multi_run_test\"\n    os.makedirs(base_dir, exist_ok=True)\n\n    # Create subdirectories for each run\n    run_dirs = [os.path.join(base_dir, f\"Run_{i+1}\") for i in range(num_runs)]\n    for run_dir in run_dirs:\n        os.makedirs(run_dir, exist_ok=True)\n\n    # Initialize SAC agent and load the pretrained model\n    sac_agent = SACAgent(input_channels=3, action_dim=4, maze_size=env_size)\n    sac_agent.actor.load_state_dict(torch.load(model_path, map_location=device))\n    print(f\"Model loaded successfully from {model_path}\")\n\n    all_rewards = []  # List to store rewards for all runs\n\n    # Perform multiple independent runs\n    for run_idx, run_dir in enumerate(run_dirs):\n        print(f\"\\nStarting run {run_idx + 1}/{num_runs}\")\n        \n        # Initialize the environment for each run\n        env = CustomMazeEnv(size=env_size, initial_obstacles=obstacles, max_obstacles=obstacles)\n        env.reset()\n\n        state = env._get_state()\n        total_reward = 0\n        rewards = []  # Cumulative rewards for this run\n\n        for step in range(max_steps):\n            # Agent selects an action\n            action = sac_agent.act(state)\n            next_state, reward, done, _ = env.step(action)\n            state = next_state\n            total_reward += reward\n            rewards.append(total_reward)  # Append cumulative reward\n\n            if done:\n                print(f\"Goal reached in {step + 1} steps with total reward {total_reward:.2f}\")\n                break\n\n        all_rewards.append(rewards)  # Store the rewards for this run\n\n    # Save performance plot\n    performance_plot_path = os.path.join(base_dir, \"Performance_Plot.png\")\n    plt.figure(figsize=(10, 5))\n    for i, rewards in enumerate(all_rewards):\n        plt.plot(rewards, label=f\"Run {i + 1}\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Cumulative Reward\")\n    plt.title(\"SAC Agent Performance Across Runs\")\n    plt.legend()\n    plt.grid()\n    plt.savefig(performance_plot_path)\n    plt.close()\n\n    print(\"Results saved in\", base_dir)\n\n\n\n\ntest_sac_model_with_multiple_runs_and_save(\"sac_model.pth\", env_size=15, obstacles=50, max_steps=800, num_runs=3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}